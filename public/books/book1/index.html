<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Scalable Monte Carlo for Bayesian Learning | Chris Nemeth</title>
<meta name="keywords" content="Bayesian, Monte Carlo, Scalable">
<meta name="description" content="This book covers recent advances in the Monte Carlo literature for performing Bayesian inference in high-dimensional and large-data settings.">
<meta name="author" content="Paul Fearnhead, Christopher Nemeth, Chris J. Oates and Chris Sherlock">
<link rel="canonical" href="https://chris-nemeth.github.io/books/book1/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.e690afcd5c523330d5c8b4d746eb158361600a015e99518d4d246a6ccab0cc19.css" integrity="sha256-5pCvzVxSMzDVyLTXRusVg2FgCgFemVGNTSRqbMqwzBk=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://chris-nemeth.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://chris-nemeth.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://chris-nemeth.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://chris-nemeth.github.io/apple-touch-icon.png">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://chris-nemeth.github.io/books/book1/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript><meta property="og:title" content="Scalable Monte Carlo for Bayesian Learning" />
<meta property="og:description" content="This book covers recent advances in the Monte Carlo literature for performing Bayesian inference in high-dimensional and large-data settings." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://chris-nemeth.github.io/books/book1/" />
<meta property="og:image" content="https://chris-nemeth.github.io/img.jpeg" /><meta property="article:section" content="books" />
<meta property="article:published_time" content="2025-06-07T00:00:00+00:00" />
<meta property="article:modified_time" content="2025-06-07T00:00:00+00:00" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://chris-nemeth.github.io/img.jpeg" />
<meta name="twitter:title" content="Scalable Monte Carlo for Bayesian Learning"/>
<meta name="twitter:description" content="This book covers recent advances in the Monte Carlo literature for performing Bayesian inference in high-dimensional and large-data settings."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Books",
      "item": "https://chris-nemeth.github.io/books/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Scalable Monte Carlo for Bayesian Learning",
      "item": "https://chris-nemeth.github.io/books/book1/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Scalable Monte Carlo for Bayesian Learning",
  "name": "Scalable Monte Carlo for Bayesian Learning",
  "description": "This book covers recent advances in the Monte Carlo literature for performing Bayesian inference in high-dimensional and large-data settings.",
  "keywords": [
    "Bayesian", "Monte Carlo", "Scalable"
  ],
  "articleBody": " Description This book provides a graduate-level introduction to advanced topics in Markov chain Monte Carlo (MCMC) algorithms, as applied broadly in the Bayesian computational context. Most of these topics, stochastic gradient MCMC, non-reversible MCMC, continuous time MCMC, and new techniques for convergence assessment, have emerged as recently as the last decade. They have driven substantial recent practical and theoretical advances in the field. A particular focus is on methods that are scalable with respect to either the amount of data, or the data dimension, motivated by the emerging high-priority application areas in machine learning and AI.\nView Full book Chapter 1: Background\nMonte Carlo Methods What is Monte Carlo Integration? Importance Sampling Monte Carlo or Quadrature? Control Variates Monte Carlo Integration and Bayesian Statistics Example Applications Logistic Regression Bayesian Matrix Factorisation Bayesian Neural Networks for Classification Markov Chains Reversible Markov chains Convergence, Averages, and Variances Stochastic Differential Equations The Ornstein–Uhlenbeck Process The Infinitesimal Generator Langevin Diffusions The Kernel Trick Finite-Dimensional Inner Product Spaces Kernels in a Finite-Dimensional Inner Product Space A New Inner Product and the Kernel Trick in Finite Dimensions General Kernels The Power of the Kernel Trick Chapter Notes Chapter 2: Reversible MCMC and its Scaling\nThe Metropolis–Hastings Algorithm Component-wise updates and Gibbs moves The Metropolis–Hastings Independence Sampler The Random Walk Metropolis Algorithm The Metropolis-Adjusted Langevin Algorithm Hamiltonian Monte Carlo Chapter Notes Chapter 3: Stochastic Gradient MCMC Algorithms\nThe Unadjusted Langevin Algorithm Approximate vs. Exact MCMC Stochastic Gradient Langevin Dynamics Controlling Stochasticity in the Gradient Estimator Example: The Value of Control Variates Convergence Results for Stochastic Gradient Langevin Dynamics A General Framework for stochastic gradient MCMC Guidance for Efficient Scalable Bayesian Learning Experiments on a Logistic Regression Model Experiments on a Bayesian Neural Network Model Generalisations and Extensions Scalable Inference for Models in Constrained Spaces Scalable Inference with Time Series Data Chapter Notes Chapter 4: Non-Reversible MCMC\nThe Benefits of Non-Reversibility Hamiltonian Monte Carlo Revisited Lifting Schemes for MCMC Non-Reversible HMC Gustafson’s Algorithm and Multidimensional Generalisations Improving Non-reversibility: Delayed Rejection The Discrete Bouncy Particle Sampler Chapter Notes Chapter 5: Continuous-Time MCMC\nContinuous-Time MCMC as the Limit of Non-Reversible MCMC Piecewise Deterministic Markov Processes What is a PDMP? Simulating PDMPs The Generator and Invariant Distribution of a PDMP The Limiting Process of Section 5.1 as a PDMP Continuous-time MCMC via PDMPs Different Samplers Use of PDMP Output Comparison of Samplers Efficient Simulation of PDMP Samplers Simulating PDMPs Exploiting Model Sparsity Data Subsampling Ideas Extensions Discontinuous Target Distribution Reversible Jump PDMP Samplers More General Velocity Models Chapter Notes Chapter 6: Assessing and Improving MCMC\nDiagnostics for MCMC Convergence Diagnostics Bias Diagnostics Improved Bias Diagnostics via the Kernel Trick Convergence Bounds for MCMC Bounds on Integral Probability Metrics Choice of Auxiliary Markov Process Kernel Stein Discrepancy Convergence Control Stochastic Gradient Stein Discrepancy Optimal Weights for MCMC Optimal Thinning for MCMC Chapter Notes Citation Fearnhead, P., Nemeth, C., Oates, C.J. and Sherlock, C., 2025. Scalable Monte Carlo for Bayesian Learning. Cambridge University Press.\n@book{fearnhead2025scalable, title={Scalable Monte Carlo for Bayesian Learning}, author={Fearnhead, Paul and Nemeth, Christopher and Oates, Chris J and Sherlock, Chris}, publisher={Cambridge University Press}, year={2025} } ",
  "wordCount" : "515",
  "inLanguage": "en",
  "image":"https://chris-nemeth.github.io/img.jpeg","datePublished": "2025-06-07T00:00:00Z",
  "dateModified": "2025-06-07T00:00:00Z",
  "author":[{
    "@type": "Person",
    "name": "Paul Fearnhead, Christopher Nemeth, Chris J. Oates and Chris Sherlock"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://chris-nemeth.github.io/books/book1/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Chris Nemeth",
    "logo": {
      "@type": "ImageObject",
      "url": "https://chris-nemeth.github.io/favicon.ico"
    }
  }
}
</script>



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" integrity="sha384-wcIxkf4k558AjM3Yz3BBFQUbk/zgIYC2R0QpeeYb+TwlBVMrlgLqwRjRtGZiK7ww" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js" integrity="sha384-hIoBPJpTUs74ddyc4bFZSM1TVlQDA60VBbJS0oA934VSz82sBx1X7kSx2ATBDIyd" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js" integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous"
  onload="renderMathInElement(document.body);"></script>

<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          delimiters: [
            {left: '$$', right: '$$', display: true},
            {left: '$', right: '$', display: false},
            {left: "\\begin{equation}", right: "\\end{equation}", display: true},
            {left: "\\begin{equation*}", right: "\\end{equation*}", display: true},
            {left: "\\begin{align}", right: "\\end{align}", display: true},
            {left: "\\begin{align*}", right: "\\end{align*}", display: true},
            {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
            {left: "\\begin{gather}", right: "\\end{gather}", display: true},
            {left: "\\begin{CD}", right: "\\end{CD}", display: true},
          ],
          throwOnError : false
        });
    });
</script>
 


</head>

<body class="" id="top">

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://chris-nemeth.github.io/" accesskey="h" title="Chris Nemeth">
                <img src="https://chris-nemeth.github.io/favicon.ico" alt="" aria-label="logo"
                    height="18"
                    width="18">Chris Nemeth</a>
            <div class="logo-switches">
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://chris-nemeth.github.io/papers/" title="Papers">
                    <span>Papers</span>
                </a>
            </li>
            <li>
                <a href="https://chris-nemeth.github.io/code_and_software/" title="Code">
                    <span>Code</span>
                </a>
            </li>
            <li>
                <a href="https://chris-nemeth.github.io/software/" title="Software">
                    <span>Software</span>
                </a>
            </li>
            <li>
                <a href="https://chris-nemeth.github.io/books/" title="Books">
                    <span>Books</span>
                </a>
            </li>
            <li>
                <a href="https://chris-nemeth.github.io/blogs/" title="Blog">
                    <span>Blog</span>
                </a>
            </li>
            <li>
                <a href="https://chris-nemeth.github.io/research_group/" title="Group">
                    <span>Group</span>
                </a>
            </li>
        </ul>
    </nav>
</header>

    <main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Scalable Monte Carlo for Bayesian Learning
    </h1>
    <div class="post-meta"><span title='2025-06-07 00:00:00 +0000 UTC'>June 2025</span>&nbsp;&middot;&nbsp;Paul Fearnhead, Christopher Nemeth, Chris J. Oates and Chris Sherlock&nbsp;&middot;&nbsp;<a href="https://www.cambridge.org/core/books/scalable-monte-carlo-for-bayesian-learning/7FDC9891DDA086B85AD8AB386F6F3BD3" rel="noopener noreferrer" target="_blank">Cambridge University Press</a>

</div>
  </header> 
  <div class="post-content"><hr>
<h4 id="description">Description<a hidden class="anchor" aria-hidden="true" href="#description">#</a></h4>
<p>This book provides a graduate-level introduction to advanced topics in Markov chain Monte Carlo (MCMC) algorithms, as applied broadly in the Bayesian computational context. Most of these topics, stochastic gradient MCMC, non-reversible MCMC, continuous time MCMC, and new techniques for convergence assessment, have emerged as recently as the last decade. They have driven substantial recent practical and theoretical advances in the field. A particular focus is on methods that are scalable with respect to either the amount of data, or the data dimension, motivated by the emerging high-priority application areas in machine learning and AI.</p>
<h4 id="view">View<a hidden class="anchor" aria-hidden="true" href="#view">#</a></h4>
<ul>
<li>
<p><a href="scalable_mcmc_book.pdf">Full book</a>
</p>
</li>
<li>
<p>Chapter 1: Background</p>
<ul>
<li>Monte Carlo Methods
<ul>
<li>What is Monte Carlo Integration?</li>
<li>Importance Sampling</li>
<li>Monte Carlo or Quadrature?</li>
<li>Control Variates</li>
<li>Monte Carlo Integration and Bayesian Statistics</li>
</ul>
</li>
<li>Example Applications
<ul>
<li>Logistic Regression</li>
<li>Bayesian Matrix Factorisation</li>
<li>Bayesian Neural Networks for Classification</li>
</ul>
</li>
<li>Markov Chains
<ul>
<li>Reversible Markov chains</li>
<li>Convergence, Averages, and Variances</li>
</ul>
</li>
<li>Stochastic Differential Equations
<ul>
<li>The Ornstein–Uhlenbeck Process</li>
<li>The Infinitesimal Generator</li>
<li>Langevin Diffusions</li>
</ul>
</li>
<li>The Kernel Trick
<ul>
<li>Finite-Dimensional Inner Product Spaces</li>
<li>Kernels in a Finite-Dimensional Inner Product Space</li>
<li>A New Inner Product and the Kernel Trick in Finite Dimensions</li>
<li>General Kernels</li>
<li>The Power of the Kernel Trick</li>
</ul>
</li>
<li>Chapter Notes</li>
</ul>
</li>
<li>
<p>Chapter 2: Reversible MCMC and its Scaling</p>
<ul>
<li>The Metropolis–Hastings Algorithm
<ul>
<li>Component-wise updates and Gibbs moves</li>
<li>The Metropolis–Hastings Independence Sampler</li>
<li>The Random Walk Metropolis Algorithm</li>
<li>The Metropolis-Adjusted Langevin Algorithm</li>
</ul>
</li>
<li>Hamiltonian Monte Carlo</li>
<li>Chapter Notes</li>
</ul>
</li>
<li>
<p>Chapter 3: Stochastic Gradient MCMC Algorithms</p>
<ul>
<li>The Unadjusted Langevin Algorithm</li>
<li>Approximate vs. Exact MCMC</li>
<li>Stochastic Gradient Langevin Dynamics
<ul>
<li>Controlling Stochasticity in the Gradient Estimator</li>
<li>Example: The Value of Control Variates</li>
<li>Convergence Results for Stochastic Gradient Langevin Dynamics</li>
</ul>
</li>
<li>A General Framework for stochastic gradient MCMC</li>
<li>Guidance for Efficient Scalable Bayesian Learning
<ul>
<li>Experiments on a Logistic Regression Model</li>
<li>Experiments on a Bayesian Neural Network Model</li>
</ul>
</li>
<li>Generalisations and Extensions
<ul>
<li>Scalable Inference for Models in Constrained Spaces</li>
<li>Scalable Inference with Time Series Data</li>
</ul>
</li>
<li>Chapter Notes</li>
</ul>
</li>
<li>
<p>Chapter 4: Non-Reversible MCMC</p>
<ul>
<li>The Benefits of Non-Reversibility</li>
<li>Hamiltonian Monte Carlo Revisited</li>
<li>Lifting Schemes for MCMC
<ul>
<li>Non-Reversible HMC</li>
<li>Gustafson’s Algorithm and Multidimensional Generalisations</li>
</ul>
</li>
<li>Improving Non-reversibility: Delayed Rejection
<ul>
<li>The Discrete Bouncy Particle Sampler</li>
</ul>
</li>
<li>Chapter Notes</li>
</ul>
</li>
<li>
<p>Chapter 5: Continuous-Time MCMC</p>
<ul>
<li>Continuous-Time MCMC as the Limit of Non-Reversible MCMC</li>
<li>Piecewise Deterministic Markov Processes
<ul>
<li>What is a PDMP?</li>
<li>Simulating PDMPs</li>
<li>The Generator and Invariant Distribution of a PDMP</li>
<li>The Limiting Process of Section 5.1 as a PDMP</li>
</ul>
</li>
<li>Continuous-time MCMC via PDMPs
<ul>
<li>Different Samplers</li>
<li>Use of PDMP Output</li>
<li>Comparison of Samplers</li>
</ul>
</li>
<li>Efficient Simulation of PDMP Samplers
<ul>
<li>Simulating PDMPs</li>
<li>Exploiting Model Sparsity</li>
<li>Data Subsampling Ideas</li>
</ul>
</li>
<li>Extensions
<ul>
<li>Discontinuous Target Distribution</li>
<li>Reversible Jump PDMP Samplers</li>
<li>More General Velocity Models</li>
</ul>
</li>
<li>Chapter Notes</li>
</ul>
</li>
<li>
<p>Chapter 6: Assessing and Improving MCMC</p>
<ul>
<li>Diagnostics for MCMC
<ul>
<li>Convergence Diagnostics</li>
<li>Bias Diagnostics</li>
<li>Improved Bias Diagnostics via the Kernel Trick</li>
</ul>
</li>
<li>Convergence Bounds for MCMC
<ul>
<li>Bounds on Integral Probability Metrics</li>
<li>Choice of Auxiliary Markov Process</li>
<li>Kernel Stein Discrepancy</li>
<li>Convergence Control</li>
<li>Stochastic Gradient Stein Discrepancy</li>
</ul>
</li>
<li>Optimal Weights for MCMC</li>
<li>Optimal Thinning for MCMC</li>
<li>Chapter Notes</li>
</ul>
</li>
</ul>
<hr>
<h4 id="citation">Citation<a hidden class="anchor" aria-hidden="true" href="#citation">#</a></h4>
<p>Fearnhead, P., Nemeth, C., Oates, C.J. and Sherlock, C., 2025. Scalable Monte Carlo for Bayesian Learning. Cambridge University Press.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-BibTeX" data-lang="BibTeX"><span style="display:flex;"><span><span style="color:#0a0;text-decoration:underline">@book</span>{fearnhead2025scalable,
</span></span><span style="display:flex;"><span>  <span style="color:#1e90ff">title</span>=<span style="color:#a50">{Scalable Monte Carlo for Bayesian Learning}</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#1e90ff">author</span>=<span style="color:#a50">{Fearnhead, Paul and Nemeth, Christopher and Oates, Chris J and Sherlock, Chris}</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#1e90ff">publisher</span>=<span style="color:#a50">{Cambridge University Press}</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#1e90ff">year</span>=<span style="color:#a50">{2025}</span>
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div>

  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://chris-nemeth.github.io/tags/bayesian/">Bayesian</a></li>
      <li><a href="https://chris-nemeth.github.io/tags/monte-carlo/">Monte Carlo</a></li>
      <li><a href="https://chris-nemeth.github.io/tags/scalable/">Scalable</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="https://chris-nemeth.github.io/">Chris Nemeth</a></span> ·     
    <span>
    Powered by 
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/pmichaillat/hugo-website/" rel="noopener" target="_blank">a modified version</a>
         of 
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>
</html>
